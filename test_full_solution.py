#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è ETL Assistant

–ú–æ–¥—É–ª–∏ 5 + 6: –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Python ETL –∫–æ–¥–∞, SQL —Å–∫—Ä–∏–ø—Ç–æ–≤ –∏ Airflow DAG
"""

import sys
import os
import json
import time
sys.path.append('backend')

from backend.data_analyzers import DataAnalyzer, StorageRecommendationEngine
from backend.etl_generators import PythonETLGenerator, SQLScriptGenerator, ConfigGenerator
from backend.airflow_generator import AirflowDAGGenerator, AirflowConfigGenerator


def test_full_etl_solution():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ ETL —Ä–µ—à–µ–Ω–∏—è –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º —Ñ–∞–π–ª–µ"""
    
    print("üöÄ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û–õ–ù–û–ì–û ETL –†–ï–®–ï–ù–ò–Ø")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    print("\nüìã –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã...")
    analyzer = DataAnalyzer()
    storage_recommender = StorageRecommendationEngine()
    etl_generator = PythonETLGenerator()
    sql_generator = SQLScriptGenerator()
    config_generator = ConfigGenerator()
    airflow_dag_generator = AirflowDAGGenerator()
    airflow_config_generator = AirflowConfigGenerator()
    
    # –ü—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤–æ–º—É —Ñ–∞–π–ª—É
    test_file = "backend/uploads/museum_data_sample.csv"
    
    if not os.path.exists(test_file):
        print(f"‚ùå –§–∞–π–ª {test_file} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ demo_analyzer.bat –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞")
        return
    
    print(f"üìÅ –¢–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª: {test_file}")
    print(f"üìè –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {os.path.getsize(test_file) / (1024*1024):.1f} MB")
    
    try:
        # –®–ê–ì 1: –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•
        print("\n" + "="*60)
        print("üìä –®–ê–ì 1: –ê–ù–ê–õ–ò–ó –°–¢–†–£–ö–¢–£–†–´ –î–ê–ù–ù–´–•")
        print("="*60)
        
        start_time = time.time()
        analysis = analyzer.analyze_file(test_file)
        analysis_time = time.time() - start_time
        
        print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {analysis_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"   üìÑ –§–æ—Ä–º–∞—Ç: {analysis.get('format_type')}")
        print(f"   üìã –°—Ç—Ä–æ–∫: {analysis.get('total_rows', 0):,}")
        print(f"   üìä –ö–æ–ª–æ–Ω–æ–∫: {analysis.get('total_columns', 0)}")
        print(f"   üóÇÔ∏è –ö–æ–¥–∏—Ä–æ–≤–∫–∞: {analysis.get('encoding')}")
        print(f"   üîó –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: '{analysis.get('separator')}'")
        
        # –®–ê–ì 2: –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –•–†–ê–ù–ò–õ–ò–©–£
        print("\n" + "="*60)
        print("üèóÔ∏è –®–ê–ì 2: –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –•–†–ê–ù–ò–õ–ò–©–£")
        print("="*60)
        
        storage_recommendation = storage_recommender.recommend_storage(analysis)
        
        print(f"üóÑÔ∏è –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {storage_recommendation['recommended_storage']}")
        print(f"üí° –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: {storage_recommendation['reasoning']}")
        print(f"üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {len(storage_recommendation['optimization_suggestions'])} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
        
        # –®–ê–ì 3: –ì–ï–ù–ï–†–ê–¶–ò–Ø ETL –ö–û–î–ê
        print("\n" + "="*60)
        print("üêç –®–ê–ì 3: –ì–ï–ù–ï–†–ê–¶–ò–Ø PYTHON ETL –ö–û–î–ê")
        print("="*60)
        
        start_time = time.time()
        etl_script = etl_generator.generate_etl_script(analysis, storage_recommendation)
        etl_generation_time = time.time() - start_time
        
        print(f"‚úÖ Python ETL —Å–∫—Ä–∏–ø—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∑–∞ {etl_generation_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"   üìù –†–∞–∑–º–µ—Ä –∫–æ–¥–∞: {len(etl_script):,} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üìÑ –°—Ç—Ä–æ–∫ –∫–æ–¥–∞: {etl_script.count(chr(10))}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º ETL —Å–∫—Ä–∏–ø—Ç
        with open('generated_etl_script.py', 'w', encoding='utf-8') as f:
            f.write(etl_script)
        print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: generated_etl_script.py")
        
        # –®–ê–ì 4: –ì–ï–ù–ï–†–ê–¶–ò–Ø SQL –°–ö–†–ò–ü–¢–û–í
        print("\n" + "="*60)
        print("üóÑÔ∏è –®–ê–ì 4: –ì–ï–ù–ï–†–ê–¶–ò–Ø SQL –°–ö–†–ò–ü–¢–û–í")
        print("="*60)
        
        sql_scripts = sql_generator.generate_transformation_sql(
            analysis, storage_recommendation['recommended_storage']
        )
        
        print(f"‚úÖ SQL —Å–∫—Ä–∏–ø—Ç—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã:")
        for script_type, script_content in sql_scripts.items():
            print(f"   üìù {script_type}: {len(script_content):,} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–π SQL —Å–∫—Ä–∏–ø—Ç
            filename = f"generated_sql_{script_type}.sql"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(script_content)
            print(f"       üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {filename}")
        
        # –®–ê–ì 5: –ì–ï–ù–ï–†–ê–¶–ò–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–û–ù–ù–´–• –§–ê–ô–õ–û–í
        print("\n" + "="*60)
        print("‚öôÔ∏è –®–ê–ì 5: –ì–ï–ù–ï–†–ê–¶–ò–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–û–ù–ù–´–• –§–ê–ô–õ–û–í")
        print("="*60)
        
        config_files = config_generator.generate_config_files(analysis, storage_recommendation)
        
        print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã:")
        for config_type, config_content in config_files.items():
            print(f"   üìù {config_type}: {len(config_content):,} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
            ext_map = {
                'environment': '.env',
                'docker_compose': '.yml', 
                'requirements': '.txt'
            }
            ext = ext_map.get(config_type, '.txt')
            
            filename = f"generated_config_{config_type}{ext}"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(config_content)
            print(f"       üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {filename}")
        
        # –®–ê–ì 6: –ì–ï–ù–ï–†–ê–¶–ò–Ø AIRFLOW DAG
        print("\n" + "="*60)
        print("‚úàÔ∏è –®–ê–ì 6: –ì–ï–ù–ï–†–ê–¶–ò–Ø AIRFLOW DAG")
        print("="*60)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ETL —à–∞–≥–∏ –¥–ª—è DAG
        etl_steps = generate_etl_steps_for_dag(analysis, storage_recommendation)
        
        start_time = time.time()
        dag_code = airflow_dag_generator.generate_dag(analysis, storage_recommendation, etl_steps)
        dag_generation_time = time.time() - start_time
        
        dag_id = airflow_dag_generator._generate_dag_id(analysis.get('filename', 'test.csv'))
        schedule = airflow_dag_generator._determine_schedule(analysis)
        
        print(f"‚úÖ Airflow DAG —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∑–∞ {dag_generation_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"   üÜî DAG ID: {dag_id}")
        print(f"   ‚è∞ –†–∞—Å–ø–∏—Å–∞–Ω–∏–µ: {schedule}")
        print(f"   üìù –†–∞–∑–º–µ—Ä –∫–æ–¥–∞: {len(dag_code):,} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üìÑ –°—Ç—Ä–æ–∫ –∫–æ–¥–∞: {dag_code.count(chr(10))}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º DAG —Ñ–∞–π–ª
        dag_filename = f"{dag_id}.py"
        with open(dag_filename, 'w', encoding='utf-8') as f:
            f.write(dag_code)
        print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {dag_filename}")
        
        # –®–ê–ì 7: –ì–ï–ù–ï–†–ê–¶–ò–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò AIRFLOW
        print("\n" + "="*60)
        print("‚öôÔ∏è –®–ê–ì 7: –ì–ï–ù–ï–†–ê–¶–ò–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò AIRFLOW")
        print("="*60)
        
        airflow_configs = airflow_config_generator.generate_airflow_configs(analysis, storage_recommendation)
        
        print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Airflow —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞:")
        for config_type, config_content in airflow_configs.items():
            print(f"   üìù {config_type}: {len(config_content):,} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
            ext_map = {
                'airflow_cfg': '.cfg',
                'docker_compose_airflow': '_airflow.yml',
                'connections_script': '.sh',
                'requirements_airflow': '_airflow.txt'
            }
            ext = ext_map.get(config_type, '.txt')
            
            filename = f"generated_{config_type}{ext}"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(config_content)
            print(f"       üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {filename}")
        
        # –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê
        print("\n" + "="*60)
        print("üéâ –ü–û–õ–ù–û–ï ETL –†–ï–®–ï–ù–ò–ï –°–ì–ï–ù–ï–†–ò–†–û–í–ê–ù–û!")
        print("="*60)
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –≤—Å–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        generated_files = [
            'generated_etl_script.py',
            dag_filename
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º SQL —Ñ–∞–π–ª—ã
        for script_type in sql_scripts.keys():
            generated_files.append(f"generated_sql_{script_type}.sql")
        
        # –î–æ–±–∞–≤–ª—è–µ–º config —Ñ–∞–π–ª—ã
        for config_type in config_files.keys():
            ext = {'environment': '.env', 'docker_compose': '.yml', 'requirements': '.txt'}.get(config_type, '.txt')
            generated_files.append(f"generated_config_{config_type}{ext}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º Airflow config —Ñ–∞–π–ª—ã
        for config_type in airflow_configs.keys():
            ext = {
                'airflow_cfg': '.cfg',
                'docker_compose_airflow': '_airflow.yml', 
                'connections_script': '.sh',
                'requirements_airflow': '_airflow.txt'
            }.get(config_type, '.txt')
            generated_files.append(f"generated_{config_type}{ext}")
        
        print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –†–ï–®–ï–ù–ò–Ø:")
        print(f"   üìÅ –ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: {analysis.get('filename')} ({analysis.get('file_size_mb', 0)} MB)")
        print(f"   üìã –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {analysis.get('total_rows', 0):,} —Å—Ç—Ä–æ–∫")
        print(f"   üóÑÔ∏è –¶–µ–ª–µ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {storage_recommendation['recommended_storage']}")
        print(f"   üìù –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(generated_files)}")
        print(f"   ‚ö° –û–±—â–µ–µ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {analysis_time + etl_generation_time + dag_generation_time:.2f} —Å–µ–∫—É–Ω–¥")
        
        print(f"\nüìÇ –°–ì–ï–ù–ï–†–ò–†–û–í–ê–ù–ù–´–ï –§–ê–ô–õ–´:")
        for i, file in enumerate(generated_files, 1):
            if os.path.exists(file):
                size = os.path.getsize(file)
                print(f"   {i:2d}. {file:40s} ({size:,} –±–∞–π—Ç)")
        
        print(f"\nüöÄ –ì–û–¢–û–í–û –ö –†–ê–ó–í–ï–†–¢–´–í–ê–ù–ò–Æ:")
        print(f"   1Ô∏è‚É£ –ó–∞–ø—É—Å—Ç–∏—Ç–µ Docker: docker-compose -f generated_config_docker_compose.yml up")
        print(f"   2Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∏—Ç–µ DAG: cp {dag_filename} ./dags/")
        print(f"   3Ô∏è‚É£ –ó–∞–ø—É—Å—Ç–∏—Ç–µ ETL: python generated_etl_script.py")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–∫—É –≤ JSON
        summary = {
            "source_file": analysis.get('filename'),
            "file_size_mb": analysis.get('file_size_mb', 0),
            "total_rows": analysis.get('total_rows', 0),
            "recommended_storage": storage_recommendation['recommended_storage'],
            "dag_id": dag_id,
            "schedule_interval": schedule,
            "generated_files": generated_files,
            "generation_time_seconds": analysis_time + etl_generation_time + dag_generation_time
        }
        
        with open('etl_solution_summary.json', 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"   üíæ –°–≤–æ–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: etl_solution_summary.json")
        
        print(f"\n‚ú® –ü–æ–ª–Ω–æ–µ ETL —Ä–µ—à–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∂—é—Ä–∏! üèÜ")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è: {str(e)}")
        import traceback
        traceback.print_exc()


def generate_etl_steps_for_dag(analysis, storage_recommendation):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —à–∞–≥–æ–≤ ETL –¥–ª—è DAG (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
    format_type = analysis.get('format_type', 'CSV')
    storage = storage_recommendation['recommended_storage']
    
    steps = [
        f"Extract: –ß—Ç–µ–Ω–∏–µ {format_type} —Ñ–∞–π–ª–∞",
        "Transform: –û—á–∏—Å—Ç–∫–∞ –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö", 
        f"Load: –ó–∞–≥—Ä—É–∑–∫–∞ –≤ {storage}",
        "Quality Check: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"
    ]
    
    return steps


if __name__ == "__main__":
    test_full_etl_solution()
