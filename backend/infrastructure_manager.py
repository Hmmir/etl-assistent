#!/usr/bin/env python3
"""
Infrastructure Manager –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è ETL —Å–∏—Å—Ç–µ–º—ã

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω ETL Assistant
–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: 2025-09-22

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ä–µ–∂–∏–º—ã:
- DEMO: –õ–æ–∫–∞–ª—å–Ω—ã–π Docker Compose (ClickHouse + PostgreSQL + Airflow)
- PRODUCTION: Yandex Cloud Managed Services
"""

import subprocess
import os
import time
import requests
import logging
from pathlib import Path
from typing import Dict, List, Tuple
import yaml
import json

logger = logging.getLogger(__name__)

class InfrastructureManager:
    def __init__(self):
        self.project_root = Path(".")
        self.demo_mode = True  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–µ–º–æ —Ä–µ–∂–∏–º
        
    def check_docker_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Docker"""
        try:
            result = subprocess.run(['docker', '--version'], 
                                 capture_output=True, text=True)
            return result.returncode == 0
        except:
            return False
    
    def check_yc_cli_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Yandex Cloud CLI"""
        try:
            result = subprocess.run(['yc', '--version'], 
                                 capture_output=True, text=True)
            return result.returncode == 0
        except:
            return False
    
    def create_demo_docker_compose(self) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ Docker Compose —Ñ–∞–π–ª–∞ –¥–ª—è –¥–µ–º–æ —Ä–µ–∂–∏–º–∞"""
        compose_content = """# ETL Assistant Demo Infrastructure
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: 2025-09-22

name: etl-assistant-demo

services:
  # ClickHouse - –æ—Å–Ω–æ–≤–Ω–æ–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
  clickhouse:
    image: yandex/clickhouse-server:latest
    container_name: etl_clickhouse
    ports:
      - "9000:9000"   # Native protocol
      - "8123:8123"   # HTTP interface
    environment:
      - CLICKHOUSE_DB=etl_data
      - CLICKHOUSE_USER=default
      - CLICKHOUSE_PASSWORD=demo123
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./init-clickhouse.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - etl_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8123/ping"]
      interval: 10s
      timeout: 5s
      retries: 3

  # PostgreSQL - —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ  
  postgres:
    image: postgres:15-alpine
    container_name: etl_postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=etl_metadata
      - POSTGRES_USER=etl_user
      - POSTGRES_PASSWORD=demo123
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-postgres.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - etl_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U etl_user -d etl_metadata"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Redis - –∫—ç—à –∏ –æ—á–µ—Ä–µ–¥–∏
  redis:
    image: redis:7-alpine
    container_name: etl_redis
    ports:
      - "6379:6379"
    networks:
      - etl_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Apache Airflow - –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è
  airflow-init:
    image: apache/airflow:2.7.2
    container_name: etl_airflow_init
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://etl_user:demo123@postgres:5432/etl_metadata
      - AIRFLOW__CORE__FERNET_KEY=YlCImzjge_TeZc7jGvnKrYjNBUeTeSEXZr-oB3nVJJ8=
      - AIRFLOW__WEBSERVER__SECRET_KEY=demo_secret_key_for_airflow
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - etl_network
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: >
      bash -c "
        airflow db init &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
      "

  airflow-webserver:
    image: apache/airflow:2.7.2
    container_name: etl_airflow_webserver
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://etl_user:demo123@postgres:5432/etl_metadata
      - AIRFLOW__CORE__FERNET_KEY=YlCImzjge_TeZc7jGvnKrYjNBUeTeSEXZr-oB3nVJJ8=
      - AIRFLOW__WEBSERVER__SECRET_KEY=demo_secret_key_for_airflow
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
    ports:
      - "8080:8080"
    depends_on:
      - airflow-init
      - postgres
      - redis
    networks:
      - etl_network
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: airflow webserver
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  airflow-scheduler:
    image: apache/airflow:2.7.2
    container_name: etl_airflow_scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://etl_user:demo123@postgres:5432/etl_metadata
      - AIRFLOW__CORE__FERNET_KEY=YlCImzjge_TeZc7jGvnKrYjNBUeTeSEXZr-oB3nVJJ8=
      - AIRFLOW__WEBSERVER__SECRET_KEY=demo_secret_key_for_airflow
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
    depends_on:
      - airflow-init
      - postgres
      - redis
    networks:
      - etl_network
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: airflow scheduler

volumes:
  clickhouse_data:
  postgres_data:

networks:
  etl_network:
    driver: bridge
"""
        
        with open("docker-compose-demo.yml", "w", encoding="utf-8") as f:
            f.write(compose_content)
        
        return "docker-compose-demo.yml"
    
    def create_init_scripts(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ SQL —Å–∫—Ä–∏–ø—Ç–æ–≤ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
        
        # ClickHouse init script
        clickhouse_init = """
-- –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ETL
CREATE DATABASE IF NOT EXISTS etl_data;

-- –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è ETL
CREATE USER IF NOT EXISTS etl_user IDENTIFIED BY 'demo123';
GRANT ALL ON etl_data.* TO etl_user;

-- –ü—Ä–∏–º–µ—Ä —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
CREATE TABLE IF NOT EXISTS etl_data.demo_table (
    id UInt64,
    name String,
    created_at DateTime,
    data String
) ENGINE = MergeTree()
ORDER BY id
PARTITION BY toYYYYMM(created_at);

-- –í—Å—Ç–∞–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
INSERT INTO etl_data.demo_table VALUES 
(1, 'Demo Record 1', now(), 'Sample data for demonstration'),
(2, 'Demo Record 2', now(), 'Another sample record');
"""
        
        with open("init-clickhouse.sql", "w", encoding="utf-8") as f:
            f.write(clickhouse_init)
        
        # PostgreSQL init script
        postgres_init = """
-- –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö ETL
CREATE SCHEMA IF NOT EXISTS airflow;
CREATE SCHEMA IF NOT EXISTS etl_metadata;

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ø–∞–π–ø–ª–∞–π–Ω–æ–≤
CREATE TABLE IF NOT EXISTS etl_metadata.pipelines (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    source_type VARCHAR(100),
    target_type VARCHAR(100),
    status VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- –í—Å—Ç–∞–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
INSERT INTO etl_metadata.pipelines (name, source_type, target_type, status) VALUES
('demo_csv_pipeline', 'CSV', 'ClickHouse', 'active'),
('demo_json_pipeline', 'JSON', 'PostgreSQL', 'inactive');
"""
        
        with open("init-postgres.sql", "w", encoding="utf-8") as f:
            f.write(postgres_init)
    
    def create_airflow_directories(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è Airflow"""
        directories = [
            "airflow/dags",
            "airflow/logs", 
            "airflow/plugins"
        ]
        
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
    
    def deploy_demo_infrastructure(self) -> Dict:
        """–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –¥–µ–º–æ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        try:
            logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –¥–µ–º–æ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º Docker
            if not self.check_docker_available():
                return {
                    "success": False,
                    "error": "Docker –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
                }
            
            # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            compose_file = self.create_demo_docker_compose()
            self.create_init_scripts()
            self.create_airflow_directories()
            
            logger.info("üìÅ –§–∞–π–ª—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω—ã")
            
            # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
            subprocess.run(['docker', 'compose', '-f', compose_file, '-p', 'etl-assistant-demo', 'down'], 
                         capture_output=True)
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É
            logger.info("üê≥ –ó–∞–ø—É—Å–∫–∞–µ–º Docker Compose...")
            result = subprocess.run(['docker', 'compose', '-f', compose_file, '-p', 'etl-assistant-demo', 'up', '-d'], 
                                  capture_output=True, text=True, encoding='utf-8')
            
            if result.returncode != 0:
                return {
                    "success": False,
                    "error": f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ Docker Compose: {result.stderr}"
                }
            
            # –ñ–¥–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–æ–≤
            logger.info("‚è≥ –û–∂–∏–¥–∞–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–æ–≤...")
            services_status = self.wait_for_services()
            
            return {
                "success": True,
                "message": "‚úÖ –î–µ–º–æ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–∞ —É—Å–ø–µ—à–Ω–æ!",
                "services": services_status,
                "endpoints": {
                    "clickhouse_http": "http://localhost:8123",
                    "postgres": "localhost:5432",
                    "airflow_ui": "http://localhost:8080",
                    "redis": "localhost:6379"
                },
                "credentials": {
                    "clickhouse": {"user": "etl_user", "password": "demo123"},
                    "postgres": {"user": "etl_user", "password": "demo123", "db": "etl_metadata"},
                    "airflow": {"user": "admin", "password": "admin"}
                }
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è: {str(e)}")
            return {
                "success": False,
                "error": f"–û—à–∏–±–∫–∞ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è: {str(e)}"
            }
    
    def wait_for_services(self, timeout: int = 120) -> Dict:
        """–û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–æ–≤"""
        services = {
            "clickhouse": {"url": "http://localhost:8123/ping", "status": "unknown"},
            "postgres": {"port": 5432, "status": "unknown"},
            "airflow": {"url": "http://localhost:8080/health", "status": "unknown"},
            "redis": {"port": 6379, "status": "unknown"}
        }
        
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            all_ready = True
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ ClickHouse
            try:
                response = requests.get("http://localhost:8123/ping", timeout=5)
                services["clickhouse"]["status"] = "ready" if response.status_code == 200 else "starting"
            except:
                services["clickhouse"]["status"] = "starting"
                all_ready = False
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ Airflow
            try:
                response = requests.get("http://localhost:8080/health", timeout=5)
                services["airflow"]["status"] = "ready" if response.status_code == 200 else "starting"
            except:
                services["airflow"]["status"] = "starting"
                all_ready = False
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ PostgreSQL –∏ Redis —á–µ—Ä–µ–∑ docker ps
            try:
                result = subprocess.run(['docker', 'ps', '--format', 'table {{.Names}}\\t{{.Status}}'], 
                                      capture_output=True, text=True)
                
                if "etl_postgres" in result.stdout and "Up" in result.stdout:
                    services["postgres"]["status"] = "ready"
                else:
                    services["postgres"]["status"] = "starting"
                    all_ready = False
                    
                if "etl_redis" in result.stdout and "Up" in result.stdout:
                    services["redis"]["status"] = "ready"
                else:
                    services["redis"]["status"] = "starting"
                    all_ready = False
            except:
                services["postgres"]["status"] = "unknown"
                services["redis"]["status"] = "unknown"
                all_ready = False
            
            if all_ready:
                break
                
            time.sleep(5)
        
        return services
    
    def get_infrastructure_status(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–ø—É—â–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞
            result = subprocess.run(['docker', 'compose', '-p', 'etl-assistant-demo', 'ps', '--format', 'json'], 
                                  capture_output=True, text=True, encoding='utf-8')
            
            containers = []
            
            if result.returncode == 0 and result.stdout.strip():
                for line in result.stdout.strip().split('\n'):
                    if line.strip():
                        try:
                            container = json.loads(line)
                            containers.append({
                                "name": container.get('Name', ''),
                                "status": container.get('Status', ''),
                                "state": container.get('State', ''),
                                "ports": container.get('Publishers', [])
                            })
                        except:
                            pass
            
            # Fallback: –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ docker ps
            if not containers:
                result = subprocess.run(['docker', 'ps', '--format', 'json'], 
                                      capture_output=True, text=True, encoding='utf-8')
                
                if result.returncode == 0:
                    for line in result.stdout.strip().split('\n'):
                        if line.strip():
                            try:
                                container = json.loads(line)
                                if any(name in container.get('Names', '') for name in ['etl_', 'airflow']):
                                    containers.append({
                                        "name": container.get('Names', ''),
                                        "status": container.get('Status', ''),
                                        "ports": container.get('Ports', '')
                                    })
                            except:
                                pass
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–æ–≤
            services_status = self.wait_for_services(timeout=10)
            
            return {
                "success": True,
                "containers": containers,
                "services": services_status,
                "demo_mode": self.demo_mode
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def stop_infrastructure(self) -> Dict:
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        try:
            compose_file = "docker-compose-demo.yml"
            
            if not Path(compose_file).exists():
                return {"success": False, "error": "Docker Compose —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω"}
            
            result = subprocess.run(['docker', 'compose', '-f', compose_file, '-p', 'etl-assistant-demo', 'down'], 
                                  capture_output=True, text=True, encoding='utf-8')
            
            if result.returncode == 0:
                return {"success": True, "message": "üõë –ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"}
            else:
                return {"success": False, "error": f"–û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏: {result.stderr}"}
                
        except Exception as e:
            return {"success": False, "error": str(e)}

    def deploy_dag_to_airflow(self, dag_content: str, dag_name: str) -> Dict:
        """–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ DAG –≤ Airflow"""
        try:
            dag_file_path = Path(f"airflow/dags/{dag_name}")
            
            with open(dag_file_path, 'w', encoding='utf-8') as f:
                f.write(dag_content)
            
            # –ñ–¥–µ–º –ø–æ–∫–∞ Airflow –ø–æ–¥—Ö–≤–∞—Ç–∏—Ç –Ω–æ–≤—ã–π DAG (–æ–±—ã—á–Ω–æ 30-60 —Å–µ–∫—É–Ω–¥)
            return {
                "success": True,
                "message": f"‚úÖ DAG {dag_name} —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç –≤ Airflow",
                "dag_file": str(dag_file_path),
                "airflow_ui": "http://localhost:8080"
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
