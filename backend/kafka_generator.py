"""
–†–µ–∞–ª—å–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä Kafka Streaming –∫–æ–¥–∞ –¥–ª—è ETL Assistant
"""

import logging
from typing import Dict, Any
from datetime import datetime

logger = logging.getLogger(__name__)


class KafkaStreamingGenerator:
    """
    –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ä–µ–∞–ª—å–Ω–æ–≥–æ Python –∫–æ–¥–∞ –¥–ª—è Kafka streaming
    """
    
    def generate_kafka_consumer(self, topic_name: str, target_storage: str, analysis: Dict[str, Any]) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π –∫–æ–¥ Kafka Consumer
        """
        
        return f'''#!/usr/bin/env python3
"""
üåä Kafka Consumer –¥–ª—è ETL Pipeline
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω ETL Assistant
–î–∞—Ç–∞: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

–¢–æ–ø–∏–∫: {topic_name}
–¶–µ–ª–µ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {target_storage}
"""

import json
import logging
from datetime import datetime
from typing import Dict, Any, List
from kafka import KafkaConsumer, KafkaProducer
from kafka.errors import KafkaError
import pandas as pd
import time
import sys

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Kafka
KAFKA_CONFIG = {{
    'bootstrap_servers': ['localhost:9092'],
    'group_id': 'etl-assistant-consumer',
    'auto_offset_reset': 'earliest',
    'value_deserializer': lambda x: json.loads(x.decode('utf-8')),
    'consumer_timeout_ms': 10000,
    'max_poll_records': 1000,
    'enable_auto_commit': True,
    'auto_commit_interval_ms': 5000
}}

# –¢–æ–ø–∏–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
INPUT_TOPIC = '{topic_name}'
ERROR_TOPIC = '{topic_name}_errors'
BATCH_SIZE = 1000

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ETLKafkaConsumer:
    """
    Kafka Consumer –¥–ª—è ETL –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è consumer"""
        try:
            self.consumer = KafkaConsumer(
                INPUT_TOPIC,
                **KAFKA_CONFIG
            )
            
            self.producer = KafkaProducer(
                bootstrap_servers=KAFKA_CONFIG['bootstrap_servers'],
                value_serializer=lambda x: json.dumps(x).encode('utf-8')
            )
            
            logger.info(f"‚úÖ Kafka Consumer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–æ–≤–∞–Ω –¥–ª—è —Ç–æ–ø–∏–∫–∞: {{INPUT_TOPIC}}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Kafka: {{str(e)}}")
            sys.exit(1)
    
    def transform_message(self, message: Dict[str, Any]) -> Dict[str, Any]:
        """
        –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏—è
        –î–æ–±–∞–≤—å—Ç–µ —Å–≤–æ—é –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫—É –∑–¥–µ—Å—å
        """
        try:
            # –ë–∞–∑–æ–≤–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è
            transformed = message.copy()
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            transformed['processed_at'] = datetime.now().isoformat()
            transformed['etl_version'] = '1.0.0'
            
            # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            for key, value in transformed.items():
                if isinstance(value, str):
                    transformed[key] = value.strip()
                elif value is None:
                    transformed[key] = ''
            
            return transformed
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏—è: {{str(e)}}")
            raise
    
    def load_to_storage(self, batch_data: List[Dict[str, Any]]) -> bool:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞—Ç—á–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ü–µ–ª–µ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        """
        try:
            logger.info(f"üíæ –ó–∞–≥—Ä—É–∂–∞–µ–º {{len(batch_data)}} –∑–∞–ø–∏—Å–µ–π –≤ {target_storage}")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
            df = pd.DataFrame(batch_data)
            
            {self._generate_storage_loader(target_storage)}
            
            logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {{len(batch_data)}} –∑–∞–ø–∏—Å–µ–π")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {{str(e)}}")
            return False
    
    def send_to_error_topic(self, message: Dict[str, Any], error: str):
        """
        –û—Ç–ø—Ä–∞–≤–∫–∞ –æ—à–∏–±–æ—á–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –≤ error —Ç–æ–ø–∏–∫
        """
        try:
            error_message = {{
                'original_message': message,
                'error': error,
                'timestamp': datetime.now().isoformat(),
                'topic': INPUT_TOPIC
            }}
            
            self.producer.send(ERROR_TOPIC, error_message)
            logger.warning(f"‚ö†Ô∏è –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ error —Ç–æ–ø–∏–∫: {{error}}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ error —Ç–æ–ø–∏–∫: {{str(e)}}")
    
    def process_stream(self):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        logger.info(f"üåä –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–æ—Ç–æ–∫–∞ –∏–∑ —Ç–æ–ø–∏–∫–∞: {{INPUT_TOPIC}}")
        
        batch = []
        batch_start_time = time.time()
        
        try:
            for message in self.consumer:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
                    raw_data = message.value
                    
                    # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
                    transformed_data = self.transform_message(raw_data)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞—Ç—á
                    batch.append(transformed_data)
                    
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–∞
                    if len(batch) >= BATCH_SIZE:
                        if self.load_to_storage(batch):
                            batch_time = time.time() - batch_start_time
                            logger.info(f"üìä –ë–∞—Ç—á –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {{batch_time:.2f}}—Å")
                        else:
                            logger.error("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞—Ç—á–∞")
                        
                        # –°–±—Ä–æ—Å –±–∞—Ç—á–∞
                        batch = []
                        batch_start_time = time.time()
                
                except Exception as e:
                    error_msg = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è: {{str(e)}}"
                    logger.error(f"‚ùå {{error_msg}}")
                    
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ error —Ç–æ–ø–∏–∫
                    try:
                        self.send_to_error_topic(message.value, error_msg)
                    except:
                        pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ error —Ç–æ–ø–∏–∫–∞
                        
        except KeyboardInterrupt:
            logger.info("üõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
            
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ—Ç–æ–∫–∞: {{str(e)}}")
            
        finally:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –¥–∞–Ω–Ω—ã–µ –≤ –±–∞—Ç—á–µ
            if batch:
                logger.info(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è {{len(batch)}} –∑–∞–ø–∏—Å–µ–π")
                self.load_to_storage(batch)
            
            self.cleanup()
    
    def cleanup(self):
        """
        –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤
        """
        try:
            if hasattr(self, 'consumer'):
                self.consumer.close()
                logger.info("‚úÖ Kafka Consumer –∑–∞–∫—Ä—ã—Ç")
                
            if hasattr(self, 'producer'):
                self.producer.close()
                logger.info("‚úÖ Kafka Producer –∑–∞–∫—Ä—ã—Ç")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π: {{str(e)}}")


def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    """
    logger.info("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º ETL Kafka Consumer")
    
    consumer = ETLKafkaConsumer()
    
    try:
        consumer.process_stream()
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {{str(e)}}")
        sys.exit(1)
    
    logger.info("‚úÖ ETL Kafka Consumer –∑–∞–≤–µ—Ä—à–µ–Ω")


if __name__ == "__main__":
    main()
'''
    
    def generate_kafka_producer(self, topic_name: str, source_config: Dict[str, Any]) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–¥ Kafka Producer –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        """
        
        return f'''#!/usr/bin/env python3
"""
üöÄ Kafka Producer –¥–ª—è ETL Pipeline
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω ETL Assistant
–î–∞—Ç–∞: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

–¢–æ–ø–∏–∫: {topic_name}
"""

import json
import logging
import pandas as pd
from datetime import datetime
from typing import Dict, Any, Iterator
from kafka import KafkaProducer
from kafka.errors import KafkaError
import time
import sys

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Kafka
KAFKA_CONFIG = {{
    'bootstrap_servers': ['localhost:9092'],
    'value_serializer': lambda x: json.dumps(x, default=str).encode('utf-8'),
    'acks': 'all',
    'retries': 3,
    'batch_size': 16384,
    'linger_ms': 10,
    'buffer_memory': 33554432
}}

OUTPUT_TOPIC = '{topic_name}'
BATCH_SIZE = 1000

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ETLKafkaProducer:
    """
    Kafka Producer –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–æ–ø–∏–∫
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è producer"""
        try:
            self.producer = KafkaProducer(**KAFKA_CONFIG)
            logger.info(f"‚úÖ Kafka Producer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–æ–≤–∞–Ω –¥–ª—è —Ç–æ–ø–∏–∫–∞: {{OUTPUT_TOPIC}}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Kafka: {{str(e)}}")
            sys.exit(1)
    
    def read_data_source(self) -> Iterator[Dict[str, Any]]:
        """
        –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (CSV, Database, API)
        """
        try:
            # –ü—Ä–∏–º–µ—Ä —á—Ç–µ–Ω–∏—è CSV —Ñ–∞–π–ª–∞
            source_file = "data.csv"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à –∏—Å—Ç–æ—á–Ω–∏–∫
            
            logger.info(f"üìÇ –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑: {{source_file}}")
            
            # –ß–∏—Ç–∞–µ–º –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã —á–∞–Ω–∫–∞–º–∏
            chunk_size = 1000
            for chunk in pd.read_csv(source_file, chunksize=chunk_size):
                for _, row in chunk.iterrows():
                    yield row.to_dict()
                    
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {{str(e)}}")
            raise
    
    def send_to_kafka(self, data: Dict[str, Any]) -> bool:
        """
        –û—Ç–ø—Ä–∞–≤–∫–∞ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏ –≤ Kafka
        """
        try:
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            data['kafka_timestamp'] = datetime.now().isoformat()
            data['producer_id'] = 'etl-assistant-producer'
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Kafka
            future = self.producer.send(OUTPUT_TOPIC, value=data)
            
            # –ñ–¥–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏)
            record_metadata = future.get(timeout=10)
            
            return True
            
        except KafkaError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Kafka: {{str(e)}}")
            return False
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {{str(e)}}")
            return False
    
    def stream_data(self):
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –¥–∞–Ω–Ω—ã—Ö
        """
        logger.info(f"üåä –ù–∞—á–∏–Ω–∞–µ–º —Å—Ç—Ä–∏–º–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–æ–ø–∏–∫: {{OUTPUT_TOPIC}}")
        
        sent_count = 0
        error_count = 0
        start_time = time.time()
        
        try:
            for data_record in self.read_data_source():
                if self.send_to_kafka(data_record):
                    sent_count += 1
                else:
                    error_count += 1
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞–∂–¥—ã–µ 1000 –∑–∞–ø–∏—Å–µ–π
                if (sent_count + error_count) % 1000 == 0:
                    elapsed = time.time() - start_time
                    rate = sent_count / elapsed if elapsed > 0 else 0
                    logger.info(f"üìä –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {{sent_count}}, –û—à–∏–±–æ–∫: {{error_count}}, –°–∫–æ—Ä–æ—Å—Ç—å: {{rate:.1f}} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫")
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_time = time.time() - start_time
            logger.info(f"‚úÖ –°—Ç—Ä–∏–º–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω:")
            logger.info(f"   üì§ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {{sent_count}} –∑–∞–ø–∏—Å–µ–π")
            logger.info(f"   ‚ùå –û—à–∏–±–æ–∫: {{error_count}}")
            logger.info(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {{total_time:.2f}} —Å–µ–∫—É–Ω–¥")
            logger.info(f"   üöÄ –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {{sent_count/total_time:.1f}} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫")
            
        except KeyboardInterrupt:
            logger.info("üõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
            
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {{str(e)}}")
            
        finally:
            self.cleanup()
    
    def cleanup(self):
        """
        –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤
        """
        try:
            if hasattr(self, 'producer'):
                self.producer.flush()  # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤—Å–µ –±—É—Ñ–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
                self.producer.close()
                logger.info("‚úÖ Kafka Producer –∑–∞–∫—Ä—ã—Ç")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ Producer: {{str(e)}}")


def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    """
    logger.info("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º ETL Kafka Producer")
    
    producer = ETLKafkaProducer()
    
    try:
        producer.stream_data()
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {{str(e)}}")
        sys.exit(1)
    
    logger.info("‚úÖ ETL Kafka Producer –∑–∞–≤–µ—Ä—à–µ–Ω")


if __name__ == "__main__":
    main()
'''

    def _generate_storage_loader(self, storage_type: str) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        """
        if storage_type.lower() == 'clickhouse':
            return '''
            # –ó–∞–≥—Ä—É–∑–∫–∞ –≤ ClickHouse
            from clickhouse_driver import Client
            
            client = Client(
                host='localhost',
                port=9000,
                database='etl_streaming'
            )
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
            columns = list(df.columns)
            data_tuples = [tuple(row) for row in df.values]
            
            # Batch insert
            client.execute(f"INSERT INTO streaming_data ({','.join(columns)}) VALUES", data_tuples)
            '''
        
        elif storage_type.lower() == 'postgresql':
            return '''
            # –ó–∞–≥—Ä—É–∑–∫–∞ –≤ PostgreSQL
            import psycopg2
            from sqlalchemy import create_engine
            
            engine = create_engine('postgresql://user:password@localhost:5432/etl_streaming')
            
            # Batch insert —Å –ø–æ–º–æ—â—å—é pandas
            df.to_sql('streaming_data', engine, if_exists='append', index=False, method='multi')
            '''
        
        else:  # HDFS
            return '''
            # –ó–∞–≥—Ä—É–∑–∫–∞ –≤ HDFS
            import hdfs3
            import pyarrow as pa
            import pyarrow.parquet as pq
            
            hdfs = hdfs3.HDFileSystem(host='localhost', port=9000)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Parquet —Ñ–æ—Ä–º–∞—Ç–µ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_path = f"/etl/streaming/data_{timestamp}.parquet"
            
            table = pa.Table.from_pandas(df)
            with hdfs.open(file_path, 'wb') as f:
                pq.write_table(table, f)
            '''

    def generate_kafka_docker_compose(self) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç Docker Compose —Ñ–∞–π–ª –¥–ª—è Kafka –∫–ª–∞—Å—Ç–µ—Ä–∞
        """
        return '''version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: etl_zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: etl_kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_HOST://localhost:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: true

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: etl_kafka_ui
    depends_on:
      - kafka
    ports:
      - "8090:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: etl-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181

networks:
  default:
    name: etl-kafka-network
'''
