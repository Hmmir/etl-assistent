"""
–ú–æ–¥—É–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Airflow DAG –¥–ª—è ETL Assistant

–ú–æ–¥—É–ª—å 6: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è DAG —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ ETL –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
"""

import os
import textwrap
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from pathlib import Path


class AirflowDAGGenerator:
    """
    –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä Apache Airflow DAG –¥–ª—è ETL –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
    """
    
    def __init__(self):
        self.default_args = {
            'owner': 'etl_assistant',
            'depends_on_past': False,
            'start_date': '2025-01-01',
            'email_on_failure': True,
            'email_on_retry': False,
            'retries': 1,
            'retry_delay': 'timedelta(minutes=5)'
        }
    
    def generate_dag(self, analysis: Dict[str, Any], storage_recommendation: Dict[str, Any], 
                    etl_steps: List[str]) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Airflow DAG –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ ETL —à–∞–≥–æ–≤
        
        Args:
            analysis: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö
            storage_recommendation: –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ö—Ä–∞–Ω–∏–ª–∏—â—É
            etl_steps: –®–∞–≥–∏ ETL –ø—Ä–æ—Ü–µ—Å—Å–∞
            
        Returns:
            str: –ì–æ—Ç–æ–≤—ã–π Python –∫–æ–¥ DAG —Ñ–∞–π–ª–∞
        """
        filename = analysis.get('filename', 'data.csv')
        storage = storage_recommendation['recommended_storage']
        total_rows = analysis.get('total_rows', analysis.get('total_records', 0))
        dag_id = self._generate_dag_id(filename)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö
        schedule = self._determine_schedule(analysis)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã DAG
        imports = self._generate_imports(storage)
        dag_definition = self._generate_dag_definition(dag_id, schedule, analysis)
        task_definitions = self._generate_task_definitions(analysis, storage_recommendation, etl_steps)
        task_dependencies = self._generate_task_dependencies(etl_steps)
        
        # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π DAG
        dag_code = f'''#!/usr/bin/env python3
"""
Airflow DAG –¥–ª—è ETL –æ–±—Ä–∞–±–æ—Ç–∫–∏ {filename}

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω ETL Assistant
–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

–ò—Å—Ç–æ—á–Ω–∏–∫: {analysis.get('format_type')} —Ñ–∞–π–ª ({analysis.get('file_size_mb', 0)} MB)
–¶–µ–ª–µ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {storage}
–°—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö: {total_rows:,}
–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ: {schedule}
"""

{imports}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
default_args = {{
    'owner': '{self.default_args["owner"]}',
    'depends_on_past': {self.default_args["depends_on_past"]},
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': {self.default_args["email_on_failure"]},
    'email_on_retry': {self.default_args["email_on_retry"]},
    'retries': {self.default_args["retries"]},
    'retry_delay': {self.default_args["retry_delay"]}
}}

{dag_definition}

{task_definitions}

{task_dependencies}
'''
        
        return dag_code
    
    def _generate_imports(self, storage: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–ª–æ–∫–∞ –∏–º–ø–æ—Ä—Ç–æ–≤"""
        imports = [
            "from datetime import datetime, timedelta",
            "from airflow import DAG",
            "from airflow.operators.python import PythonOperator",
            "from airflow.operators.bash import BashOperator", 
            "from airflow.operators.email import EmailOperator",
            "import pandas as pd",
            "import logging",
            "import os"
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∏–º–ø–æ—Ä—Ç—ã
        if storage == 'ClickHouse':
            imports.extend([
                "from airflow.providers.http.operators.http import SimpleHttpOperator",
                "from clickhouse_driver import Client"
            ])
        elif storage == 'PostgreSQL':
            imports.extend([
                "from airflow.providers.postgres.operators.postgres import PostgresOperator",
                "from airflow.providers.postgres.hooks.postgres import PostgresHook"
            ])
        elif storage == 'HDFS':
            imports.extend([
                "from airflow.providers.apache.hdfs.sensors.hdfs import HdfsSensor",
                "import hdfs3"
            ])
        
        return "\n".join(imports)
    
    def _generate_dag_definition(self, dag_id: str, schedule: str, analysis: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è DAG"""
        description = f"ETL –ø—Ä–æ—Ü–µ—Å—Å –¥–ª—è {analysis.get('filename', '—Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö')}"
        
        return f'''
# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ DAG
dag = DAG(
    dag_id='{dag_id}',
    description='{description}',
    default_args=default_args,
    schedule_interval='{schedule}',
    catchup=False,
    tags=['etl', 'auto-generated', '{analysis.get("format_type", "data").lower()}'],
    max_active_runs=1
)
'''
    
    def _generate_task_definitions(self, analysis: Dict[str, Any], storage_recommendation: Dict[str, Any], 
                                  etl_steps: List[str]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –∑–∞–¥–∞—á"""
        filename = analysis.get('filename', 'data.csv')
        storage = storage_recommendation['recommended_storage']
        
        tasks = []
        
        # –í–ê–ñ–ù–û: FileSensor –ù–ï –ò–°–ü–û–õ–¨–ó–£–ï–¢–°–Ø, —Ç–∞–∫ –∫–∞–∫ —Ñ–∞–π–ª –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —Ö–æ—Å—Ç-—Å–∏—Å—Ç–µ–º–µ
        # –î–ª—è production: –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ volume mapping –≤ docker-compose:
        #   volumes:
        #     - C:/Users/alien/Desktop/etl/backend/uploads:/data
        
        # 1. –ó–∞–¥–∞—á–∞ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞ (–∑–∞–º–µ–Ω—è–µ—Ç FileSensor)
        tasks.append(f'''
def start_etl_task(**context):
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ETL –ø—Ä–æ—Ü–µ—Å—Å–∞ (–≤–º–µ—Å—Ç–æ FileSensor)
    """
    logger = logging.getLogger(__name__)
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ ETL –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–ª—è {filename}")
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ DAG run
    conf = context.get('dag_run').conf or {{}}
    source_file = conf.get('source_file', '/data/{filename}')
    
    logger.info(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: {{source_file}}")
    
    return {{
        "source_file": source_file,
        "etl_started": True,
        "start_time": str(datetime.now())
    }}

start_etl = PythonOperator(
    task_id='start_etl',
    python_callable=start_etl_task,
    dag=dag
)''')
        
        # 2. –ó–∞–¥–∞—á–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö
        tasks.append(f'''
def analyze_data_task(**context):
    """
    –ó–∞–¥–∞—á–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    """
    logger = logging.getLogger(__name__)
    ti = context['ti']
    start_result = ti.xcom_pull(task_ids='start_etl')
    file_path = start_result.get('source_file', '/data/{filename}')
    
    logger.info(f"–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞: {{file_path}}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
    if os.path.exists(file_path):
        file_size = os.path.getsize(file_path)
        logger.info(f"‚úÖ –§–∞–π–ª –Ω–∞–π–¥–µ–Ω, —Ä–∞–∑–º–µ—Ä: {{file_size:,}} –±–∞–π—Ç")
    else:
        # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        logger.warning(f"‚ö†Ô∏è –§–∞–π–ª {{file_path}} –Ω–µ –Ω–∞–π–¥–µ–Ω –ª–æ–∫–∞–ª—å–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º API")
        file_size = 0
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö –∑–∞–¥–∞—á
    return {{
        "file_path": file_path,
        "file_size": file_size,
        "analysis_completed": True
    }}

analyze_data = PythonOperator(
    task_id='analyze_data',
    python_callable=analyze_data_task,
    dag=dag
)''')
        
        # 3. –ó–∞–¥–∞—á–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (Extract)
        extract_task = self._generate_extract_task(analysis, storage)
        tasks.append(extract_task)
        
        # 4. –ó–∞–¥–∞—á–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö (Transform)  
        transform_task = self._generate_transform_task(analysis)
        tasks.append(transform_task)
        
        # 5. –ó–∞–¥–∞—á–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö (Load)
        load_task = self._generate_load_task(analysis, storage_recommendation)
        tasks.append(load_task)
        
        # 6. –ó–∞–¥–∞—á–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        quality_task = self._generate_quality_check_task(analysis, storage)
        tasks.append(quality_task)
        
        # 7. –ó–∞–¥–∞—á–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ–± —É—Å–ø–µ—à–Ω–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
        tasks.append('''
# –ó–∞–¥–∞—á–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ–± —É—Å–ø–µ—à–Ω–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
success_notification = EmailOperator(
    task_id='success_notification',
    to=['admin@company.com'],
    subject='ETL –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ: {{ ds }}',
    html_content="""
    <h3>ETL –ø—Ä–æ—Ü–µ—Å—Å —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω</h3>
    <p><strong>–î–∞—Ç–∞:</strong> {{ ds }}</p>
    <p><strong>DAG:</strong> {{ dag.dag_id }}</p>
    <p><strong>–°—Ç–∞—Ç—É—Å:</strong> SUCCESS</p>
    """,
    dag=dag
)''')
        
        return "\n".join(tasks)
    
    def _generate_extract_task(self, analysis: Dict[str, Any], storage: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        format_type = analysis.get('format_type', 'CSV')
        filename = analysis.get('filename', 'data.csv')
        
        if format_type == 'CSV':
            encoding = analysis.get('encoding', 'utf-8')
            separator = analysis.get('separator', ',')
            
            return f'''
def extract_csv_data(**context):
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV —Ñ–∞–π–ª–∞
    """
    logger = logging.getLogger(__name__)
    logger.info("–ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV")
    
    file_path = "/data/{filename}"
    
    try:
        # –ß–∏—Ç–∞–µ–º CSV —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        df = pd.read_csv(
            file_path,
            sep='{separator}',
            encoding='{encoding}',
            low_memory=False
        )
        
        logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {{len(df):,}} —Å—Ç—Ä–æ–∫, {{len(df.columns)}} –∫–æ–ª–æ–Ω–æ–∫")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∑–∞–¥–∞—á–∏
        temp_path = "/tmp/extracted_data.parquet"
        df.to_parquet(temp_path)
        
        return {{
            "temp_file": temp_path,
            "rows_extracted": len(df),
            "columns_count": len(df.columns)
        }}
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {{str(e)}}")
        raise

extract_data = PythonOperator(
    task_id='extract_data',
    python_callable=extract_csv_data,
    dag=dag
)'''
        
        elif format_type == 'JSON':
            return '''
def extract_json_data(**context):
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON —Ñ–∞–π–ª–∞
    """
    logger = logging.getLogger(__name__)
    logger.info("–ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON")
    
    file_path = "/data/''' + filename + '''"
    
    try:
        df = pd.read_json(file_path)
        logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(df):,} –∑–∞–ø–∏—Å–µ–π")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ
        temp_path = "/tmp/extracted_data.parquet"
        df.to_parquet(temp_path)
        
        return {
            "temp_file": temp_path,
            "rows_extracted": len(df)
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        raise

extract_data = PythonOperator(
    task_id='extract_data',
    python_callable=extract_json_data,
    dag=dag
)'''
        
        else:  # XML –∏–ª–∏ –¥—Ä—É–≥–∏–µ —Ñ–æ—Ä–º–∞—Ç—ã
            return '''
def extract_data_generic(**context):
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞
    """
    logger = logging.getLogger(__name__)
    logger.info("–ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö")
    
    # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –ª–æ–≥–∏–∫–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
    # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—ã–∑–æ–≤ ETL —Å–∫—Ä–∏–ø—Ç–∞
    
    return {"extraction_completed": True}

extract_data = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data_generic,
    dag=dag
)'''
    
    def _generate_transform_task(self, analysis: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
        columns = analysis.get('columns', analysis.get('fields', []))
        has_nulls = any(col.get('null_count', 0) > 0 for col in columns)
        has_datetime = any(col.get('data_type') == 'datetime' for col in columns)
        
        cleaning_logic = []
        if has_nulls:
            cleaning_logic.append("        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
            cleaning_logic.append("        df = df.dropna(how='all')  # –£–¥–∞–ª—è–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏")
            
        if has_datetime:
            cleaning_logic.append("        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞—Ç")
            cleaning_logic.append("        datetime_cols = [col for col in df.columns if 'date' in col.lower()]")
            cleaning_logic.append("        for col in datetime_cols:")
            cleaning_logic.append("            df[col] = pd.to_datetime(df[col], errors='coerce')")
        
        cleaning_code = "\\n".join(cleaning_logic) if cleaning_logic else "        # –î–∞–Ω–Ω—ã–µ –Ω–µ —Ç—Ä–µ–±—É—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏"
        
        return f'''
def transform_data_task(**context):
    """
    –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    """
    logger = logging.getLogger(__name__)
    logger.info("–ù–∞—á–∏–Ω–∞–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö")
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–¥–∞—á–∏
    ti = context['ti']
    extract_result = ti.xcom_pull(task_ids='extract_data')
    temp_file = extract_result.get('temp_file')
    
    try:
        # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = pd.read_parquet(temp_file)
        original_rows = len(df)
        
{cleaning_code}
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–æ–ª—è
        df['etl_processed_at'] = pd.Timestamp.now()
        df['etl_batch_id'] = context['run_id']
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        transformed_path = "/tmp/transformed_data.parquet"
        df.to_parquet(transformed_path)
        
        logger.info(f"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {{original_rows}} ‚Üí {{len(df)}} —Å—Ç—Ä–æ–∫")
        
        return {{
            "transformed_file": transformed_path,
            "rows_processed": len(df),
            "transformation_completed": True
        }}
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {{str(e)}}")
        raise

transform_data = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data_task,
    dag=dag
)'''
    
    def _generate_load_task(self, analysis: Dict[str, Any], storage_recommendation: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        storage = storage_recommendation['recommended_storage']
        table_name = self._get_table_name(analysis)
        
        if storage == 'ClickHouse':
            return f'''
def load_to_clickhouse_task(**context):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ ClickHouse
    """
    logger = logging.getLogger(__name__)
    logger.info("–ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö –≤ ClickHouse")
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    ti = context['ti']
    transform_result = ti.xcom_pull(task_ids='transform_data')
    transformed_file = transform_result.get('transformed_file')
    
    try:
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse
        client = Client(
            host=os.getenv('CLICKHOUSE_HOST', 'localhost'),
            port=int(os.getenv('CLICKHOUSE_PORT', 9000)),
            database=os.getenv('CLICKHOUSE_DB', 'default')
        )
        
        # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = pd.read_parquet(transformed_file)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
        table_exists = client.execute(
            "SELECT count() FROM system.tables WHERE name = %(table)s",
            {{'table': '{table_name}'}}
        )[0][0] > 0
        
        if not table_exists:
            logger.info("–¢–∞–±–ª–∏—Ü–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç - —Å–æ–∑–¥–∞–µ–º")
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç DDL –∏–∑ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data_tuples = [tuple(row) for row in df.values]
        placeholders = "(" + ",".join(["%s"] * len(df.columns)) + ")"
        
        insert_query = f"INSERT INTO {table_name} VALUES {{placeholders}}"
        client.execute(insert_query, data_tuples)
        
        logger.info(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {{len(df):,}} –∑–∞–ø–∏—Å–µ–π –≤ ClickHouse")
        
        return {{
            "rows_loaded": len(df),
            "target_table": "{table_name}",
            "load_completed": True
        }}
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ ClickHouse: {{str(e)}}")
        raise

load_data = PythonOperator(
    task_id='load_data',
    python_callable=load_to_clickhouse_task,
    dag=dag
)'''
        
        elif storage == 'PostgreSQL':
            return f'''
def load_to_postgresql_task(**context):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ PostgreSQL
    """
    logger = logging.getLogger(__name__)
    logger.info("–ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö –≤ PostgreSQL")
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    ti = context['ti']
    transform_result = ti.xcom_pull(task_ids='transform_data')
    transformed_file = transform_result.get('transformed_file')
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Airflow PostgresHook
        postgres_hook = PostgresHook(postgres_conn_id='postgres_default')
        
        # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = pd.read_parquet(transformed_file)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ PostgreSQL
        df.to_sql(
            name='{table_name}',
            con=postgres_hook.get_sqlalchemy_engine(),
            if_exists='append',
            index=False,
            chunksize=1000
        )
        
        logger.info(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {{len(df):,}} –∑–∞–ø–∏—Å–µ–π –≤ PostgreSQL")
        
        return {{
            "rows_loaded": len(df),
            "target_table": "{table_name}",
            "load_completed": True
        }}
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ PostgreSQL: {{str(e)}}")
        raise

load_data = PythonOperator(
    task_id='load_data',
    python_callable=load_to_postgresql_task,
    dag=dag
)'''
        
        else:  # HDFS –∏–ª–∏ –¥—Ä—É–≥–∏–µ
            return '''
def load_to_storage_task(**context):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ü–µ–ª–µ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    """
    logger = logging.getLogger(__name__)
    logger.info("–ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö")
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    ti = context['ti']
    transform_result = ti.xcom_pull(task_ids='transform_data')
    
    # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –ª–æ–≥–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    
    return {"load_completed": True}

load_data = PythonOperator(
    task_id='load_data',
    python_callable=load_to_storage_task,
    dag=dag
)'''
    
    def _generate_quality_check_task(self, analysis: Dict[str, Any], storage: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        return '''
def quality_check_task(**context):
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    logger = logging.getLogger(__name__)
    logger.info("–ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö")
    
    # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–≥—Ä—É–∑–∫–∏
    ti = context['ti']
    load_result = ti.xcom_pull(task_ids='load_data')
    rows_loaded = load_result.get('rows_loaded', 0)
    
    # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
    if rows_loaded == 0:
        raise ValueError("–ù–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏")
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∑–¥–µ—Å—å
    logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–π–¥–µ–Ω–∞: {rows_loaded:,} –∑–∞–ø–∏—Å–µ–π")
    
    return {
        "quality_check_passed": True,
        "rows_validated": rows_loaded
    }

quality_check = PythonOperator(
    task_id='quality_check',
    python_callable=quality_check_task,
    dag=dag
)'''
    
    def _generate_task_dependencies(self, etl_steps: List[str]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏"""
        # FileSensor –£–ë–†–ê–ù - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è start_etl –≤–º–µ—Å—Ç–æ –Ω–µ–≥–æ
        return '''
# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏
# FileSensor –∑–∞–º–µ–Ω–µ–Ω –Ω–∞ start_etl –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã –±–µ–∑ file sensor
start_etl >> analyze_data >> extract_data >> transform_data >> load_data >> quality_check >> success_notification
'''
    
    def _generate_dag_id(self, filename: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è ID –¥–ª—è DAG"""
        # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        file_path = Path(filename)
        name_without_ext = file_path.stem.lower()
        extension = file_path.suffix.lower().replace('.', '')  # .csv -> csv
        
        # –û—á–∏—â–∞–µ–º –∏–º—è –æ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        dag_id = name_without_ext.replace('-', '_').replace(' ', '_')
        dag_id = ''.join(c for c in dag_id if c.isalnum() or c == '_')
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ü–µ
        if extension:
            dag_id = f"{dag_id}_{extension}"
        
        return dag_id if dag_id else "etl_data_processing"
    
    def _get_table_name(self, analysis: Dict[str, Any]) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ —Ç–∞–±–ª–∏—Ü—ã"""
        filename = analysis.get('filename', 'data.csv')
        table_name = Path(filename).stem.lower()
        table_name = table_name.replace('-', '_').replace(' ', '_')
        table_name = ''.join(c for c in table_name if c.isalnum() or c == '_')
        return table_name or 'etl_data'
    
    def _determine_schedule(self, analysis: Dict[str, Any]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–µ—Ä–µ–¥–∞–Ω –ª–∏ —É–∂–µ schedule —è–≤–Ω–æ
        if 'schedule' in analysis:
            return analysis['schedule']
        
        data_volume = analysis.get('estimated_data_volume', 'medium')
        file_size_mb = analysis.get('file_size_mb', 0)
        total_rows = analysis.get('total_rows', analysis.get('total_records', 0))
        
        # –õ–æ–≥–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∞—Å—Ç–æ—Ç—ã –∑–∞–ø—É—Å–∫–∞
        if data_volume == 'small' or file_size_mb < 10 or total_rows < 10000:
            return '@daily'  # –ï–∂–µ–¥–Ω–µ–≤–Ω–æ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        elif data_volume == 'medium' or file_size_mb < 100:
            return '@hourly'   # –ö–∞–∂–¥—ã–π —á–∞—Å –¥–ª—è —Å—Ä–µ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö (—Ç—Ä–µ–±—É—é—Ç —á–∞—Å—Ç–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è)
        elif data_volume == 'large' or file_size_mb < 1000:
            return '0 2 * * *'  # –í 2:00 –∫–∞–∂–¥—ã–π –¥–µ–Ω—å –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        else:
            return '0 1 * * 0'  # –ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω–æ –≤ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ –≤ 1:00 –¥–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö


class AirflowConfigGenerator:
    """
    –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è Airflow
    """
    
    def generate_airflow_configs(self, analysis: Dict[str, Any], storage_recommendation: Dict[str, Any]) -> Dict[str, str]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è Airflow
        
        Returns:
            dict: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        """
        configs = {
            'airflow_cfg': self._generate_airflow_cfg(),
            'docker_compose_airflow': self._generate_airflow_docker_compose(storage_recommendation),
            'connections_script': self._generate_connections_script(storage_recommendation),
            'requirements_airflow': self._generate_airflow_requirements(storage_recommendation)
        }
        
        return configs
    
    def _generate_airflow_cfg(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Airflow"""
        return '''[core]
# –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Airflow –¥–ª—è ETL Assistant
dags_folder = /opt/airflow/dags
base_log_folder = /opt/airflow/logs
logging_level = INFO
executor = LocalExecutor
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow
load_examples = False
max_active_runs_per_dag = 1

[webserver]
base_url = http://localhost:8080
web_server_port = 8080
authenticate = True
auth_backend = airflow.contrib.auth.backends.password_auth

[scheduler]
dag_dir_list_interval = 300
catchup_by_default = False
max_threads = 2

[email]
email_backend = airflow.utils.email.send_email_smtp
smtp_host = smtp.gmail.com
smtp_starttls = True
smtp_ssl = False
smtp_port = 587'''
    
    def _generate_airflow_docker_compose(self, storage_recommendation: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è docker-compose –¥–ª—è Airflow"""
        storage = storage_recommendation['recommended_storage']
        
        compose = f'''version: '3.8'

# Airflow + ETL Services
# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

x-airflow-common: &airflow-common
  image: apache/airflow:2.5.0
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
    - AIRFLOW__CORE__LOAD_EXAMPLES=false
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./data:/data
  depends_on:
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres_db_volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    ports:
      - "5432:5432"

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "${{HOSTNAME}}"']
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always

  airflow-init:
    <<: *airflow-common
    command: version
    environment:
      - _AIRFLOW_DB_UPGRADE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin
'''
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–µ—Ä–≤–∏—Å—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        if storage == 'ClickHouse':
            compose += '''
  clickhouse:
    image: yandex/clickhouse-server:latest
    ports:
      - "9000:9000"
      - "8123:8123"
    volumes:
      - clickhouse_data:/var/lib/clickhouse
'''
        
        elif storage == 'PostgreSQL':
            compose += '''
  postgres-data:
    image: postgres:13
    environment:
      - POSTGRES_USER=etl_user
      - POSTGRES_PASSWORD=etl_password
      - POSTGRES_DB=etl_data
    ports:
      - "5433:5432"
    volumes:
      - postgres_etl_data:/var/lib/postgresql/data
'''
        
        compose += '''
volumes:
  postgres_db_volume:'''
        
        if storage == 'ClickHouse':
            compose += '\n  clickhouse_data:'
        elif storage == 'PostgreSQL':
            compose += '\n  postgres_etl_data:'
        
        return compose
    
    def _generate_connections_script(self, storage_recommendation: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π Airflow"""
        storage = storage_recommendation['recommended_storage']
        
        script = '''#!/bin/bash
# –°–∫—Ä–∏–ø—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π Airflow –¥–ª—è ETL Assistant

# –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞ Airflow
sleep 30

echo "–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è Airflow..."
'''
        
        if storage == 'ClickHouse':
            script += '''
# ClickHouse –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
airflow connections add 'clickhouse_default' \\
    --conn-type 'http' \\
    --conn-host 'clickhouse' \\
    --conn-port 8123 \\
    --conn-extra '{"protocol": "http"}'
'''
        
        elif storage == 'PostgreSQL':
            script += '''
# PostgreSQL –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
airflow connections add 'postgres_data' \\
    --conn-type 'postgres' \\
    --conn-host 'postgres-data' \\
    --conn-port 5432 \\
    --conn-login 'etl_user' \\
    --conn-password 'etl_password' \\
    --conn-schema 'etl_data'
'''
        
        script += '''
# –§–∞–π–ª–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
airflow connections add 'fs_default' \\
    --conn-type 'fs' \\
    --conn-extra '{"path": "/data"}'

echo "–ü–æ–¥–∫–ª—é—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!"
'''
        
        return script
    
    def _generate_airflow_requirements(self, storage_recommendation: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è requirements.txt –¥–ª—è Airflow"""
        storage = storage_recommendation['recommended_storage']
        
        requirements = [
            "# Airflow ETL Requirements",
            f"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "apache-airflow==2.5.0",
            "pandas>=1.5.0",
            "numpy>=1.21.0"
        ]
        
        if storage == 'ClickHouse':
            requirements.extend([
                "clickhouse-driver>=0.2.0",
                "apache-airflow-providers-http>=4.0.0"
            ])
        elif storage == 'PostgreSQL':
            requirements.extend([
                "psycopg2-binary>=2.9.0",
                "apache-airflow-providers-postgres>=5.0.0"
            ])
        elif storage == 'HDFS':
            requirements.extend([
                "hdfs3>=0.3.0",
                "pyarrow>=8.0.0",
                "apache-airflow-providers-apache-hdfs>=3.0.0"
            ])
        
        return "\n".join(requirements)
