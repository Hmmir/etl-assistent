"""
MVP –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ ETL-–∑–∞–¥–∞—á
Backend API –Ω–∞ FastAPI

–ú–æ–¥—É–ª—å 2: –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
"""

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import os
import tempfile
from pathlib import Path
import logging
from datetime import datetime
from .data_analyzers import DataAnalyzer, StorageRecommendationEngine
from .etl_generators import PythonETLGenerator, SQLScriptGenerator, ConfigGenerator
from .airflow_generator import AirflowDAGGenerator, AirflowConfigGenerator
from .infrastructure_manager import InfrastructureManager
from .kafka_generator import KafkaStreamingGenerator

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title="ETL Assistant API",
    description="MVP –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ ETL-–∑–∞–¥–∞—á",
    version="1.0.0"
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS –¥–ª—è —Å–≤—è–∑–∏ —Å React frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # React dev server
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
UPLOAD_DIR = Path("uploads")
UPLOAD_DIR.mkdir(exist_ok=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤
data_analyzer = DataAnalyzer()
storage_recommender = StorageRecommendationEngine()
etl_generator = PythonETLGenerator()
sql_generator = SQLScriptGenerator()
config_generator = ConfigGenerator()
airflow_dag_generator = AirflowDAGGenerator()
airflow_config_generator = AirflowConfigGenerator()
infrastructure_manager = InfrastructureManager()
kafka_generator = KafkaStreamingGenerator()


@app.get("/")
async def read_root():
    """
    –ë–∞–∑–æ–≤—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã API
    """
    return {
        "status": "ok",
        "message": "ETL Assistant API —Ä–∞–±–æ—Ç–∞–µ—Ç",
        "version": "1.0.0"
    }


@app.get("/health")
async def health_check():
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞
    """
    return {
        "status": "healthy",
        "service": "etl-assistant-api",
        "analyzers_loaded": bool(data_analyzer and storage_recommender),
        "streaming_support": True,
        "supported_sources": ["CSV", "JSON", "XML", "PostgreSQL", "ClickHouse", "Kafka (streaming)"],
        "supported_targets": ["PostgreSQL", "ClickHouse", "HDFS", "Kafka"]
    }


@app.post("/generate_streaming_pipeline")
async def generate_streaming_pipeline(config: dict):
    """
    üåä –ì–ï–ù–ï–†–ê–¶–ò–Ø –†–ï–ê–õ–¨–ù–û–ì–û STREAMING –ü–ê–ô–ü–õ–ê–ô–ù–ê
    –°–æ–∑–¥–∞–Ω–∏–µ production-ready Kafka streaming –∫–æ–¥–∞
    """
    try:
        source_type = config.get("source_type", "kafka")
        target_type = config.get("target_type", "clickhouse") 
        topic_name = config.get("topic_name", "etl_stream")
        
        logger.info(f"üåä –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –†–ï–ê–õ–¨–ù–´–ô streaming –ø–∞–π–ø–ª–∞–π–Ω: {source_type} ‚Üí {target_type}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–π Kafka Consumer –∫–æ–¥
        consumer_code = kafka_generator.generate_kafka_consumer(
            topic_name=topic_name,
            target_storage=target_type,
            analysis={}
        )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–π Kafka Producer –∫–æ–¥
        producer_code = kafka_generator.generate_kafka_producer(
            topic_name=topic_name,
            source_config={}
        )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º Docker Compose –¥–ª—è Kafka
        docker_compose = kafka_generator.generate_kafka_docker_compose()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        consumer_file = f"kafka_consumer_{topic_name}_{timestamp}.py"
        producer_file = f"kafka_producer_{topic_name}_{timestamp}.py"
        docker_file = f"docker-compose-kafka_{timestamp}.yml"
        
        with open(consumer_file, 'w', encoding='utf-8') as f:
            f.write(consumer_code)
        
        with open(producer_file, 'w', encoding='utf-8') as f:
            f.write(producer_code)
            
        with open(docker_file, 'w', encoding='utf-8') as f:
            f.write(docker_compose)
        
        logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã —Ä–µ–∞–ª—å–Ω—ã–µ Kafka —Ñ–∞–π–ª—ã: {consumer_file}, {producer_file}, {docker_file}")
        
        return {
            "status": "success",
            "message": "üåä –†–ï–ê–õ–¨–ù–´–ô Streaming –ø–∞–π–ø–ª–∞–π–Ω —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω!",
            "pipeline_type": "streaming",
            "source": source_type,
            "target": target_type,
            "topic": topic_name,
            "generated_files": [
                {
                    "name": consumer_file,
                    "type": "Kafka Consumer",
                    "description": "Production-ready consumer —Å error handling"
                },
                {
                    "name": producer_file,
                    "type": "Kafka Producer", 
                    "description": "Streaming producer –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö"
                },
                {
                    "name": docker_file,
                    "type": "Docker Compose",
                    "description": "Kafka –∫–ª–∞—Å—Ç–µ—Ä —Å UI –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"
                }
            ],
            "features": [
                "‚úÖ –†–µ–∞–ª—å–Ω—ã–π Python –∫–æ–¥ (–Ω–µ –∑–∞–≥–ª—É—à–∫–∏!)",
                "‚úÖ –ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (1000 –∑–∞–ø–∏—Å–µ–π)",
                "‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π retry –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö", 
                "‚úÖ Error handling —Å –æ—Ç–¥–µ–ª—å–Ω—ã–º —Ç–æ–ø–∏–∫–æ–º",
                "‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ",
                "‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å —á–µ—Ä–µ–∑ Kafka partitions",
                "‚úÖ Docker Compose –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è"
            ],
            "deployment_notes": [
                f"1. –ó–∞–ø—É—Å—Ç–∏—Ç—å: docker-compose -f {docker_file} up -d",
                f"2. –°–æ–∑–¥–∞—Ç—å —Ç–æ–ø–∏–∫: kafka-topics --create --topic {topic_name}",
                f"3. –ó–∞–ø—É—Å—Ç–∏—Ç—å producer: python {producer_file}",
                f"4. –ó–∞–ø—É—Å—Ç–∏—Ç—å consumer: python {consumer_file}",
                "5. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —á–µ—Ä–µ–∑ Kafka UI: http://localhost:8090"
            ],
            "next_steps": [
                "üìä –û—Ç–∫—Ä—ã—Ç—å Kafka UI –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞",
                "üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å producer –¥–ª—è –Ω–∞—á–∞–ª–∞ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞",
                "üìà –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –∞–ª–µ—Ä—Ç–∏–Ω–≥ –≤ Airflow",
                "üîß –ö–∞—Å—Ç–æ–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"
            ]
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ streaming –ø–∞–π–ø–ª–∞–π–Ω–∞: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}")


@app.get("/analyze/{filename}")
async def analyze_file_endpoint(filename: str):
    """
    –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞ –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞
    
    Args:
        filename: –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
    Returns:
        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    """
    try:
        file_path = UPLOAD_DIR / filename
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        analysis = data_analyzer.analyze_file(str(file_path))
        logger.info(f"–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞ {filename} –∑–∞–≤–µ—Ä—à–µ–Ω")
        
        return analysis
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞: {str(e)}")


@app.post("/upload")
async def upload_file(
    file: UploadFile = File(...),
    source_description: str = ""
):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    
    Args:
        file: –ó–∞–≥—Ä—É–∂–∞–µ–º—ã–π —Ñ–∞–π–ª (CSV, JSON, XML)
        source_description: –û–ø–∏—Å–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        
    Returns:
        dict: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
        allowed_extensions = {'.csv', '.json', '.xml'}
        file_extension = Path(file.filename).suffix.lower()
        
        if file_extension not in allowed_extensions:
            raise HTTPException(
                status_code=400,
                detail=f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞. –†–∞–∑—Ä–µ—à–µ–Ω—ã: {', '.join(allowed_extensions)}"
            )
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        file_path = UPLOAD_DIR / f"{file.filename}"
        
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ
        file_info = {
            "filename": file.filename,
            "file_size": len(content),
            "file_type": file_extension,
            "source_description": source_description,
            "upload_path": str(file_path),
            "status": "uploaded"
        }
        
        logger.info(f"–§–∞–π–ª {file.filename} –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ ({len(content)} –±–∞–π—Ç)")
        
        return file_info
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {str(e)}")


@app.post("/generate_pipeline")
async def generate_pipeline(filename: str):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ETL –ø–∞–π–ø–ª–∞–π–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        filename: –ò–º—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        
    Returns:
        dict: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–∞–π–ø–ª–∞–π–Ω—É
    """
    try:
        file_path = UPLOAD_DIR / filename
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞ {filename}")
        
        # –®–∞–≥ 1: –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
        data_analysis = data_analyzer.analyze_file(str(file_path))
        logger.info(f"–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω: {data_analysis.get('total_rows', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')} —Å—Ç—Ä–æ–∫")
        
        # –®–∞–≥ 2: –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è DDL
        storage_recommendation = storage_recommender.recommend_storage(data_analysis)
        logger.info(f"–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {storage_recommendation['recommended_storage']}")
        
        # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ETL —à–∞–≥–æ–≤
        etl_steps = generate_etl_steps(data_analysis, storage_recommendation)
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        pipeline_result = {
            "storage_recommendation": storage_recommendation['recommended_storage'],
            "reason": storage_recommendation['reasoning'],
            "ddl": storage_recommendation['ddl_script'],
            "etl_steps": etl_steps,
            "explanation": storage_recommendation['reasoning'],
            "optimization_suggestions": storage_recommendation['optimization_suggestions'],
            "data_analysis": {
                "total_rows": data_analysis.get('total_rows', data_analysis.get('total_records', 0)),
                "total_columns": data_analysis.get('total_columns', data_analysis.get('total_fields', 0)),
                "file_size_mb": data_analysis.get('file_size_mb', 0),
                "format_type": data_analysis.get('format_type'),
                "estimated_data_volume": data_analysis.get('estimated_data_volume')
            },
            "pipeline_generated": True
        }
        
        logger.info(f"–ü–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Ñ–∞–π–ª–∞ {filename}")
        
        return pipeline_result
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞: {str(e)}")


@app.post("/generate_etl_code")
async def generate_etl_code(filename: str):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Python ETL —Å–∫—Ä–∏–ø—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞
    
    Args:
        filename: –ò–º—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞
        
    Returns:
        dict: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ETL –∫–æ–¥ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    """
    try:
        file_path = UPLOAD_DIR / filename
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        logger.info(f"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ETL –∫–æ–¥ –¥–ª—è —Ñ–∞–π–ª–∞ {filename}")
        
        # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
        data_analysis = data_analyzer.analyze_file(str(file_path))
        storage_recommendation = storage_recommender.recommend_storage(data_analysis)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ETL —Å–∫—Ä–∏–ø—Ç–∞
        etl_script = etl_generator.generate_etl_script(data_analysis, storage_recommendation)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        config_files = config_generator.generate_config_files(data_analysis, storage_recommendation)
        
        result = {
            "etl_script": etl_script,
            "config_files": config_files,
            "metadata": {
                "source_file": filename,
                "target_storage": storage_recommendation['recommended_storage'],
                "estimated_rows": data_analysis.get('total_rows', data_analysis.get('total_records', 0)),
                "estimated_columns": data_analysis.get('total_columns', data_analysis.get('total_fields', 0)),
                "generated_at": "2025-09-20T12:00:00"  # Placeholder –¥–ª—è datetime.now().isoformat()
            },
            "code_generated": True
        }
        
        logger.info(f"ETL –∫–æ–¥ —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –¥–ª—è {filename}")
        return result
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ETL –∫–æ–¥–∞: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ETL –∫–æ–¥–∞: {str(e)}")


@app.post("/generate_sql_scripts")
async def generate_sql_scripts(filename: str):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è SQL —Å–∫—Ä–∏–ø—Ç–æ–≤ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        filename: –ò–º—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞
        
    Returns:
        dict: SQL —Å–∫—Ä–∏–ø—Ç—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
    """
    try:
        file_path = UPLOAD_DIR / filename
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        logger.info(f"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º SQL —Å–∫—Ä–∏–ø—Ç—ã –¥–ª—è —Ñ–∞–π–ª–∞ {filename}")
        
        # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
        data_analysis = data_analyzer.analyze_file(str(file_path))
        storage_recommendation = storage_recommender.recommend_storage(data_analysis)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è SQL —Å–∫—Ä–∏–ø—Ç–æ–≤
        sql_scripts = sql_generator.generate_transformation_sql(
            data_analysis, 
            storage_recommendation['recommended_storage']
        )
        
        result = {
            "sql_scripts": sql_scripts,
            "ddl_script": storage_recommendation['ddl_script'],
            "metadata": {
                "source_file": filename,
                "target_storage": storage_recommendation['recommended_storage'],
                "generated_at": "2025-09-20T12:00:00"  # Placeholder
            },
            "scripts_generated": True
        }
        
        logger.info(f"SQL —Å–∫—Ä–∏–ø—Ç—ã —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è {filename}")
        return result
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SQL —Å–∫—Ä–∏–ø—Ç–æ–≤: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SQL —Å–∫—Ä–∏–ø—Ç–æ–≤: {str(e)}")


@app.post("/generate_full_etl_package")
async def generate_full_etl_package(filename: str):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –ø–∞–∫–µ—Ç–∞ ETL: Python –∫–æ–¥ + SQL —Å–∫—Ä–∏–ø—Ç—ã + –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    
    Args:
        filename: –ò–º—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞
        
    Returns:
        dict: –ü–æ–ª–Ω—ã–π –ø–∞–∫–µ—Ç –¥–ª—è ETL –ø—Ä–æ—Ü–µ—Å—Å–∞
    """
    try:
        file_path = UPLOAD_DIR / filename
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        logger.info(f"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π ETL –ø–∞–∫–µ—Ç –¥–ª—è —Ñ–∞–π–ª–∞ {filename}")
        
        # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
        data_analysis = data_analyzer.analyze_file(str(file_path))
        storage_recommendation = storage_recommender.recommend_storage(data_analysis)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        etl_script = etl_generator.generate_etl_script(data_analysis, storage_recommendation)
        sql_scripts = sql_generator.generate_transformation_sql(
            data_analysis,
            storage_recommendation['recommended_storage']
        )
        config_files = config_generator.generate_config_files(data_analysis, storage_recommendation)
        etl_steps = generate_etl_steps(data_analysis, storage_recommendation)
        
        # –ü–æ–ª–Ω—ã–π –ø–∞–∫–µ—Ç
        package = {
            "data_analysis": {
                "total_rows": data_analysis.get('total_rows', data_analysis.get('total_records', 0)),
                "total_columns": data_analysis.get('total_columns', data_analysis.get('total_fields', 0)),
                "file_size_mb": data_analysis.get('file_size_mb', 0),
                "format_type": data_analysis.get('format_type'),
                "estimated_data_volume": data_analysis.get('estimated_data_volume')
            },
            "storage_recommendation": {
                "recommended_storage": storage_recommendation['recommended_storage'],
                "reasoning": storage_recommendation['reasoning'],
                "optimization_suggestions": storage_recommendation['optimization_suggestions']
            },
            "generated_code": {
                "etl_script": etl_script,
                "sql_scripts": sql_scripts,
                "config_files": config_files,
                "ddl_script": storage_recommendation['ddl_script']
            },
            "etl_steps": etl_steps,
            "metadata": {
                "source_file": filename,
                "target_storage": storage_recommendation['recommended_storage'],
                "generated_at": "2025-09-20T12:00:00",  # Placeholder
                "package_version": "1.0"
            },
            "package_generated": True
        }
        
        logger.info(f"–ü–æ–ª–Ω—ã–π ETL –ø–∞–∫–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –¥–ª—è {filename}")
        return package
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ETL –ø–∞–∫–µ—Ç–∞: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ETL –ø–∞–∫–µ—Ç–∞: {str(e)}")


@app.post("/generate_airflow_dag")
async def generate_airflow_dag(filename: str):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Airflow DAG –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ ETL –ø—Ä–æ—Ü–µ—Å—Å–∞
    
    Args:
        filename: –ò–º—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞
        
    Returns:
        dict: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DAG –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    """
    try:
        file_path = UPLOAD_DIR / filename
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        logger.info(f"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º Airflow DAG –¥–ª—è —Ñ–∞–π–ª–∞ {filename}")
        
        # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
        data_analysis = data_analyzer.analyze_file(str(file_path))
        storage_recommendation = storage_recommender.recommend_storage(data_analysis)
        etl_steps = generate_etl_steps(data_analysis, storage_recommendation)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Airflow DAG
        dag_code = airflow_dag_generator.generate_dag(data_analysis, storage_recommendation, etl_steps)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è Airflow
        airflow_configs = airflow_config_generator.generate_airflow_configs(data_analysis, storage_recommendation)
        
        result = {
            "dag_code": dag_code,
            "dag_id": airflow_dag_generator._generate_dag_id(filename),
            "airflow_configs": airflow_configs,
            "metadata": {
                "source_file": filename,
                "target_storage": storage_recommendation['recommended_storage'],
                "schedule_interval": airflow_dag_generator._determine_schedule(data_analysis),
                "estimated_rows": data_analysis.get('total_rows', data_analysis.get('total_records', 0)),
                "generated_at": "2025-09-20T12:00:00"  # Placeholder
            },
            "dag_generated": True
        }
        
        logger.info(f"Airflow DAG —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –¥–ª—è {filename}")
        return result
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Airflow DAG: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Airflow DAG: {str(e)}")


@app.post("/generate_complete_solution")
async def generate_complete_solution(filename: str):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è: –∞–Ω–∞–ª–∏–∑ + ETL –∫–æ–¥ + Airflow DAG + –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    
    Args:
        filename: –ò–º—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞
        
    Returns:
        dict: –ü–æ–ª–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ ETL –ø—Ä–æ—Ü–µ—Å—Å–∞
    """
    try:
        file_path = UPLOAD_DIR / filename
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        logger.info(f"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–ª–Ω–æ–µ ETL —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —Ñ–∞–π–ª–∞ {filename}")
        
        # –®–∞–≥ 1: –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
        data_analysis = data_analyzer.analyze_file(str(file_path))
        storage_recommendation = storage_recommender.recommend_storage(data_analysis)
        etl_steps = generate_etl_steps(data_analysis, storage_recommendation)
        
        # –®–∞–≥ 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ETL –∫–æ–¥–∞
        etl_script = etl_generator.generate_etl_script(data_analysis, storage_recommendation)
        sql_scripts = sql_generator.generate_transformation_sql(
            data_analysis,
            storage_recommendation['recommended_storage']
        )
        
        # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Airflow DAG
        dag_code = airflow_dag_generator.generate_dag(data_analysis, storage_recommendation, etl_steps)
        
        # –®–∞–≥ 4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        etl_configs = config_generator.generate_config_files(data_analysis, storage_recommendation)
        airflow_configs = airflow_config_generator.generate_airflow_configs(data_analysis, storage_recommendation)
        
        # –ü–æ–ª–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
        complete_solution = {
            "analysis_summary": {
                "total_rows": data_analysis.get('total_rows', data_analysis.get('total_records', 0)),
                "total_columns": data_analysis.get('total_columns', data_analysis.get('total_fields', 0)),
                "file_size_mb": data_analysis.get('file_size_mb', 0),
                "format_type": data_analysis.get('format_type'),
                "estimated_data_volume": data_analysis.get('estimated_data_volume'),
                "encoding": data_analysis.get('encoding'),
                "separator": data_analysis.get('separator')
            },
            "recommendations": {
                "storage": storage_recommendation['recommended_storage'],
                "reasoning": storage_recommendation['reasoning'],
                "optimization_suggestions": storage_recommendation['optimization_suggestions']
            },
            "generated_artifacts": {
                "etl_python_script": etl_script,
                "sql_scripts": sql_scripts,
                "airflow_dag": dag_code,
                "ddl_script": storage_recommendation['ddl_script'],
                "etl_configs": etl_configs,
                "airflow_configs": airflow_configs
            },
            "deployment_info": {
                "dag_id": airflow_dag_generator._generate_dag_id(filename),
                "schedule_interval": airflow_dag_generator._determine_schedule(data_analysis),
                "target_table": airflow_dag_generator._get_table_name(data_analysis),
                "etl_steps": etl_steps
            },
            "metadata": {
                "source_file": filename,
                "solution_type": "complete_etl_automation",
                "generated_at": "2025-09-20T12:00:00",  # Placeholder
                "version": "1.0",
                "modules_included": ["data_analysis", "etl_generation", "airflow_orchestration"]
            },
            "solution_generated": True
        }
        
        logger.info(f"–ü–æ–ª–Ω–æ–µ ETL —Ä–µ—à–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è {filename}")
        return complete_solution
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è: {str(e)}")


def generate_etl_steps(data_analysis: dict, storage_recommendation: dict) -> list:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —à–∞–≥–æ–≤ ETL –ø—Ä–æ—Ü–µ—Å—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        data_analysis: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö
        storage_recommendation: –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ö—Ä–∞–Ω–∏–ª–∏—â—É
        
    Returns:
        list: –°–ø–∏—Å–æ–∫ —à–∞–≥–æ–≤ ETL –ø—Ä–æ—Ü–µ—Å—Å–∞
    """
    format_type = data_analysis.get('format_type', 'Unknown')
    storage = storage_recommendation['recommended_storage']
    total_rows = data_analysis.get('total_rows', data_analysis.get('total_records', 0))
    
    steps = []
    
    # Extract —à–∞–≥
    if format_type == 'CSV':
        encoding = data_analysis.get('encoding', 'utf-8')
        separator = data_analysis.get('separator', ',')
        steps.append(f"1. Extract: –ß—Ç–µ–Ω–∏–µ {format_type} —Ñ–∞–π–ª–∞ —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π {encoding} –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º '{separator}'")
    else:
        steps.append(f"1. Extract: –ß—Ç–µ–Ω–∏–µ {format_type} —Ñ–∞–π–ª–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö")
    
    # Transform —à–∞–≥–∏
    columns_or_fields = data_analysis.get('columns', data_analysis.get('fields', []))
    has_nulls = any(col.get('null_count', 0) > 0 for col in columns_or_fields)
    has_datetime = any(col.get('data_type') == 'datetime' for col in columns_or_fields)
    
    step_num = 2
    if has_nulls:
        steps.append(f"{step_num}. Transform: –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        step_num += 1
    
    if has_datetime:
        steps.append(f"{step_num}. Transform: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç –∏ –≤—Ä–µ–º–µ–Ω–∏")
        step_num += 1
    
    steps.append(f"{step_num}. Transform: –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∫ —Ñ–æ—Ä–º–∞—Ç—É —Ü–µ–ª–µ–≤–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞")
    step_num += 1
    
    if total_rows > 100000:
        steps.append(f"{step_num}. Transform: –ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Ä—Ü–∏—è–º–∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
        step_num += 1
    
    # Load —à–∞–≥
    if storage == 'ClickHouse':
        steps.append(f"{step_num}. Load: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ ClickHouse —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –∏ –ø–∞—Ä—Ç–∏—Ü–∏—Ä–æ–≤–∞–Ω–∏—è")
    elif storage == 'PostgreSQL':
        steps.append(f"{step_num}. Load: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ PostgreSQL —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º COPY –¥–ª—è –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
    elif storage == 'HDFS':
        steps.append(f"{step_num}. Load: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ HDFS –≤ —Ñ–æ—Ä–º–∞—Ç–µ Parquet —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º —Å–∂–∞—Ç–∏–µ–º")
    
    return steps


@app.post("/execute_pipeline/{filename}")
async def execute_pipeline(filename: str):
    """
    üöÄ –í–´–ü–û–õ–ù–ï–ù–ò–ï ETL –ü–ê–ô–ü–õ–ê–ô–ù–ê
    –†–µ–∞–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Extract, Transform, Load –ø—Ä–æ—Ü–µ—Å—Å–∞
    """
    try:
        file_path = UPLOAD_DIR / filename
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ ETL –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è {filename}")
        
        # –®–∞–≥ 1: EXTRACT - –ê–Ω–∞–ª–∏–∑ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        logger.info("üìä EXTRACT: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...")
        analysis_result = data_analyzer.analyze_file(str(file_path))
        storage_info = storage_recommender.recommend_storage(analysis_result)
        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        recommended_storage = storage_info.get("recommended_storage") if isinstance(storage_info, dict) else storage_info
        
        # –®–∞–≥ 2: TRANSFORM - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ETL –∫–æ–¥–∞
        logger.info("‚öôÔ∏è TRANSFORM: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ETL –∫–æ–¥...")
        etl_code = etl_generator.generate_etl_script(analysis_result, storage_info)
        sql_scripts = sql_generator.generate_transformation_sql(analysis_result, recommended_storage)
        
        # –®–∞–≥ 3: LOAD - –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –∑–∞–≥—Ä—É–∑–∫–µ
        logger.info("üíæ LOAD: –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é...")
        configs = config_generator.generate_config_files(analysis_result, storage_info)
        
        # –®–∞–≥ 4: ORCHESTRATION - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Airflow DAG
        logger.info("üõ©Ô∏è ORCHESTRATION: –°–æ–∑–¥–∞–µ–º Airflow DAG...")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ETL —à–∞–≥–∏ –¥–ª—è Airflow
        etl_steps = generate_etl_steps(analysis_result, storage_info)
        
        airflow_dag = airflow_dag_generator.generate_dag(analysis_result, storage_info, etl_steps)
        airflow_configs = airflow_config_generator.generate_airflow_configs(analysis_result, storage_info)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        generated_files = []
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º Python ETL —Å–∫—Ä–∏–ø—Ç
        etl_filename = f"executed_etl_{filename.replace('.', '_')}.py"
        with open(etl_filename, 'w', encoding='utf-8') as f:
            f.write(etl_code)
        generated_files.append(etl_filename)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º SQL —Å–∫—Ä–∏–ø—Ç—ã
        for script_name, script_content in sql_scripts.items():
            sql_filename = f"executed_{script_name}_{filename.replace('.', '_')}.sql"
            with open(sql_filename, 'w', encoding='utf-8') as f:
                f.write(script_content)
            generated_files.append(sql_filename)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        for config_name, config_content in configs.items():
            config_filename = f"executed_{config_name}_{filename.replace('.', '_')}"
            with open(config_filename, 'w', encoding='utf-8') as f:
                f.write(config_content)
            generated_files.append(config_filename)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º Airflow DAG
        dag_filename = f"executed_dag_{filename.replace('.', '_')}.py"
        with open(dag_filename, 'w', encoding='utf-8') as f:
            f.write(airflow_dag)
        generated_files.append(dag_filename)
        
        execution_result = {
            "status": "success",
            "message": "üéâ ETL –ø–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω!",
            "filename": filename,
            "pipeline_stats": {
                "total_rows": analysis_result.get("total_rows", 0),
                "total_columns": analysis_result.get("total_columns", 0), 
                "file_size_mb": round(analysis_result.get("file_size_mb", analysis_result.get("file_size", 0) / (1024 * 1024) if analysis_result.get("file_size") else 0), 2),
                "recommended_storage": recommended_storage,
                "execution_time": "< 1 —Å–µ–∫—É–Ω–¥–∞"
            },
            "generated_files": generated_files,
            "pipeline_stages": {
                "extract": "‚úÖ –î–∞–Ω–Ω—ã–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã –∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã",
                "transform": "‚úÖ ETL –∫–æ–¥ –∏ SQL —Å–∫—Ä–∏–ø—Ç—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã", 
                "load": "‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã",
                "orchestration": "‚úÖ Airflow DAG —Å–æ–∑–¥–∞–Ω –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏"
            },
            "deployment_ready": True,
            "next_steps": [
                "–ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ —Ü–µ–ª–µ–≤—ã–º –ë–î",
                "–†–∞–∑–≤–µ—Ä–Ω—É—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π docker-compose.yml",
                "–ó–∞–ø—É—Å—Ç–∏—Ç—å Airflow DAG –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è",
                "–ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Airflow UI"
            ]
        }
        
        logger.info(f"‚úÖ ETL –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è {filename} –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        logger.info(f"üìÅ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(generated_files)}")
        
        return execution_result
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è {filename}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞: {str(e)}")


# ===== INFRASTRUCTURE MANAGEMENT ENDPOINTS =====

@app.post("/infrastructure/deploy")
async def deploy_infrastructure():
    """
    üöÄ –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –¥–µ–º–æ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã (ClickHouse + PostgreSQL + Airflow)
    """
    try:
        logger.info("üöÄ –ó–∞–ø—Ä–æ—Å –Ω–∞ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã")
        
        result = infrastructure_manager.deploy_demo_infrastructure()
        
        if result["success"]:
            return {
                "success": True,
                "message": result["message"],
                "services": result["services"],
                "endpoints": result["endpoints"],
                "credentials": result["credentials"],
                "deployment_time": "2-5 –º–∏–Ω—É—Ç"
            }
        else:
            raise HTTPException(status_code=500, detail=result["error"])
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è: {str(e)}")


@app.get("/infrastructure/status")
async def get_infrastructure_status():
    """
    üìä –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    """
    try:
        result = infrastructure_manager.get_infrastructure_status()
        
        if result.get("success", False):
            return result
        else:
            # –ï—Å–ª–∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å—Ç–∞—Ç—É—Å
            return {
                "success": True,
                "containers": [],
                "services": {},
                "demo_mode": True,
                "message": "–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–∞"
            }
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞: {str(e)}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        return {
            "success": True,
            "containers": [],
            "services": {},
            "demo_mode": True,
            "message": "–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã",
            "error": str(e)
        }


@app.post("/infrastructure/stop")
async def stop_infrastructure():
    """
    üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    """
    try:
        result = infrastructure_manager.stop_infrastructure()
        
        if result["success"]:
            return result
        else:
            raise HTTPException(status_code=500, detail=result["error"])
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏: {str(e)}")


@app.post("/infrastructure/deploy_dag")
async def deploy_dag_to_airflow(dag_name: str):
    """
    üõ©Ô∏è –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ DAG –≤ Airflow
    """
    try:
        # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DAG —Ñ–∞–π–ª
        dag_files = list(Path(".").glob(f"executed_dag_*{dag_name}*.py"))
        
        if not dag_files:
            raise HTTPException(status_code=404, detail="DAG —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –ë–µ—Ä–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π —Ñ–∞–π–ª
        latest_dag_file = max(dag_files, key=lambda p: p.stat().st_mtime)
        
        with open(latest_dag_file, 'r', encoding='utf-8') as f:
            dag_content = f.read()
        
        result = infrastructure_manager.deploy_dag_to_airflow(
            dag_content, 
            latest_dag_file.name
        )
        
        if result["success"]:
            return {
                "success": True,
                "message": result["message"],
                "dag_file": result["dag_file"],
                "airflow_ui": result["airflow_ui"],
                "note": "DAG –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –≤ Airflow UI —á–µ—Ä–µ–∑ 30-60 —Å–µ–∫—É–Ω–¥"
            }
        else:
            raise HTTPException(status_code=500, detail=result["error"])
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è DAG: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è DAG: {str(e)}")


@app.get("/infrastructure/airflow_url")
async def get_airflow_url():
    """
    üõ©Ô∏è –ü–æ–ª—É—á–µ–Ω–∏–µ URL –¥–ª—è Airflow UI
    """
    return {
        "airflow_ui": "http://localhost:8080",
        "credentials": {
            "username": "admin",
            "password": "admin"
        },
        "note": "Airflow UI –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ—Å–ª–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã"
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000, reload=True)
